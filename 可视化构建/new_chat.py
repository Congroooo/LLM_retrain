import gradio as gr
import os
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å… TRANSFORMERS_CACHE è­¦å‘Š
os.environ['HF_HOME'] = os.environ.get('HF_HOME', os.path.expanduser('~/.cache/huggingface'))

# åœ¨è¿™é‡ŒæŒ‡å®šåŒ…å«å¤šä¸ªæ¨¡å‹æ–‡ä»¶å¤¹çš„æ ¹è·¯å¾„
MODELS_ROOT_PATH = "E:/llama_factory/LLaMA-Factory/output_model"  # è¯·ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ¨¡å‹æ ¹è·¯å¾„

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œtokenizer
model = None
tokenizer = None
current_model_name = None
conversation_history = []


def get_available_models():
    """è·å–æŒ‡å®šè·¯å¾„ä¸‹çš„æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    models = []

    if not os.path.exists(MODELS_ROOT_PATH):
        return ["é”™è¯¯ï¼šæ¨¡å‹æ ¹è·¯å¾„ä¸å­˜åœ¨"]

    if not os.path.isdir(MODELS_ROOT_PATH):
        return ["é”™è¯¯ï¼šæ¨¡å‹æ ¹è·¯å¾„ä¸æ˜¯ç›®å½•"]

    try:
        # éå†æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for item in os.listdir(MODELS_ROOT_PATH):
            item_path = os.path.join(MODELS_ROOT_PATH, item)

            if os.path.isdir(item_path):
                # æ£€æŸ¥å­æ–‡ä»¶å¤¹æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                model_files = []
                try:
                    model_files = os.listdir(item_path)
                except:
                    continue

                # æ£€æŸ¥å¸¸è§çš„æ¨¡å‹æ–‡ä»¶æ‰©å±•å
                model_extensions = ('.bin', '.safetensors', '.pt', '.pth', '.msgpack')
                has_model_files = any(fname.endswith(model_extensions) for fname in model_files)

                # æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®æ–‡ä»¶
                has_config = any(
                    fname in ['config.json', 'pytorch_model.bin', 'model.safetensors'] for fname in model_files)

                if has_model_files or has_config:
                    models.append(item)

        # æŒ‰åç§°æ’åº
        models.sort()

        if not models:
            models = ["æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹"]

        return models

    except Exception as e:
        return [f"æ‰«ææ¨¡å‹æ—¶å‡ºé”™: {str(e)}"]


def load_model(selected_model):
    """åŠ è½½é€‰å®šçš„æ¨¡å‹"""
    global model, tokenizer, current_model_name, conversation_history

    if selected_model in ["æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹", "é”™è¯¯ï¼šæ¨¡å‹æ ¹è·¯å¾„ä¸å­˜åœ¨",
                          "é”™è¯¯ï¼šæ¨¡å‹æ ¹è·¯å¾„ä¸æ˜¯ç›®å½•"] or selected_model.startswith("é”™è¯¯ï¼š"):
        return f"é”™è¯¯ï¼š{selected_model}", ""

    model_path = os.path.join(MODELS_ROOT_PATH, selected_model)

    if not os.path.exists(model_path):
        return f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ '{model_path}' ä¸å­˜åœ¨", ""

    try:
        # å¸è½½ä¹‹å‰çš„æ¨¡å‹
        if model is not None:
            del model
            del tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # æ¸…ç©ºå¯¹è¯å†å²
        conversation_history = []

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {selected_model}")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")

        # æ£€æŸ¥è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")

        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )

        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )

        # å¦‚æœä½¿ç”¨CPUï¼Œæ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        if device == "cpu":
            model = model.to(device)

        current_model_name = selected_model

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        conversation_history.append({
            "role": "system",
            "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
        })

        return f"æ¨¡å‹åŠ è½½æˆåŠŸï¼\næ¨¡å‹: {selected_model}\nè·¯å¾„: {model_path}\nè®¾å¤‡: {device}\nå¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼", ""

    except Exception as e:
        return f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}", ""


def unload_model():
    """å¸è½½å½“å‰æ¨¡å‹"""
    global model, tokenizer, current_model_name, conversation_history

    if model is None:
        return "æ²¡æœ‰æ¨¡å‹éœ€è¦å¸è½½", ""

    try:
        model_name = current_model_name
        # é‡Šæ”¾æ¨¡å‹å†…å­˜
        del model
        del tokenizer
        model = None
        tokenizer = None
        current_model_name = None
        conversation_history = []

        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return f"æ¨¡å‹ '{model_name}' å·²æˆåŠŸå¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾", ""

    except Exception as e:
        return f"å¸è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}", ""


def chat_with_model(message, chat_history):
    """ä¸æ¨¡å‹å¯¹è¯"""
    global model, tokenizer, conversation_history

    if model is None:
        return chat_history, "é”™è¯¯ï¼šè¯·å…ˆåŠ è½½æ¨¡å‹ï¼"

    if not message or not message.strip():
        return chat_history, "é”™è¯¯ï¼šè¯·è¾“å…¥æœ‰æ•ˆæ¶ˆæ¯ï¼"

    try:
        # ç«‹å³å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²
        chat_history.append({"role": "user", "content": message})
        yield chat_history, ""

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        conversation_history.append({"role": "user", "content": message.strip()})

        # å‡†å¤‡è¾“å…¥
        text = tokenizer.apply_chat_template(
            conversation_history,
            tokenize=False,
            add_generation_prompt=True
        )

        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        # æ˜¾ç¤ºæ€è€ƒä¸­çŠ¶æ€
        chat_history.append({"role": "assistant", "content": "ğŸ¤” æ€è€ƒä¸­..."})
        yield chat_history, ""

        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.5,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )

        # æå–æ–°ç”Ÿæˆçš„token
        input_length = model_inputs.input_ids.shape[1]
        generated_ids = generated_ids[:, input_length:]

        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        conversation_history.append({"role": "assistant", "content": response})

        # ç§»é™¤æ€è€ƒä¸­æ¶ˆæ¯
        chat_history.pop()

        # é€å­—æ˜¾ç¤ºå›å¤
        displayed_response = ""
        for i in range(len(response)):
            displayed_response = response[:i + 1]
            chat_history.append({"role": "assistant", "content": displayed_response + "â–Œ"})
            yield chat_history, ""
            time.sleep(0.02)  # æ§åˆ¶é€å­—æ˜¾ç¤ºé€Ÿåº¦
            chat_history.pop()  # ç§»é™¤ä¸´æ—¶æ¶ˆæ¯

        # æœ€ç»ˆæ˜¾ç¤ºå®Œæ•´å›å¤
        chat_history.append({"role": "assistant", "content": displayed_response})
        yield chat_history, ""

    except Exception as e:
        error_msg = f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"
        print(error_msg)
        # ç§»é™¤æ€è€ƒä¸­æ¶ˆæ¯å¹¶æ˜¾ç¤ºé”™è¯¯
        if chat_history and chat_history[-1].get("content") == "ğŸ¤” æ€è€ƒä¸­...":
            chat_history.pop()
        chat_history.append({"role": "assistant", "content": f"âŒ {error_msg}"})
        yield chat_history, error_msg


def clear_history():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    global conversation_history

    conversation_history = []
    # é‡æ–°æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
    if model is not None:
        conversation_history.append({
            "role": "system",
            "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
        })

    return [], "å¯¹è¯å†å²å·²æ¸…ç©ºï¼"


def refresh_models():
    """åˆ·æ–°æ¨¡å‹åˆ—è¡¨"""
    models = get_available_models()
    return gr.Dropdown(choices=models, value=models[0] if models else "")


def get_root_path_info():
    """è·å–æ ¹è·¯å¾„ä¿¡æ¯"""
    return f"æ¨¡å‹æ ¹è·¯å¾„: {MODELS_ROOT_PATH}"


# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="ä¸­åŒ»è¯çŸ¥è¯†é—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("#ä¸­åŒ»è¯çŸ¥è¯†é—®ç­”ç³»ç»Ÿ")
    gr.Markdown("### å®æ—¶å¯¹è¯ä½“éªŒ - é—®é¢˜å³æ—¶æ˜¾ç¤ºï¼Œå›å¤é€å­—è¾“å‡º")

    # æ˜¾ç¤ºæ ¹è·¯å¾„ä¿¡æ¯
    path_info = gr.Textbox(
        value=get_root_path_info(),
        label="è·¯å¾„é…ç½®",
        interactive=False
    )

    with gr.Row():
        with gr.Column(scale=3):
            model_dropdown = gr.Dropdown(
                choices=get_available_models(),
                label="é€‰æ‹©æ¨¡å‹",
                info="ä»ä¸‹æ‹‰æ¡†ä¸­é€‰æ‹©è¦åŠ è½½çš„æ¨¡å‹"
            )
        with gr.Column(scale=1):
            refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm")

    with gr.Row():
        load_btn = gr.Button("âœ… åŠ è½½æ¨¡å‹", variant="primary")
        unload_btn = gr.Button("âŒ å¸è½½æ¨¡å‹", variant="stop")
        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", variant="secondary")

    status_display = gr.Textbox(
        label="çŠ¶æ€ä¿¡æ¯",
        interactive=False,
        lines=4,
        placeholder="æ¨¡å‹çŠ¶æ€å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
    )

    # ä½¿ç”¨æ–°çš„ messages æ ¼å¼ï¼Œä¿®å¤å¼ƒç”¨è­¦å‘Š
    chatbot = gr.Chatbot(
        label="å¯¹è¯å†…å®¹",
        height=400,
        placeholder="åŠ è½½æ¨¡å‹åï¼Œåœ¨è¿™é‡Œå¼€å§‹å¯¹è¯...",
        type="messages",  # ä½¿ç”¨æ–°çš„ messages æ ¼å¼
        show_copy_button=True
    )

    with gr.Row():
        msg = gr.Textbox(
            label="è¾“å…¥æ¶ˆæ¯",
            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
            scale=4,
            max_lines=3
        )
        submit_btn = gr.Button("å‘é€", variant="primary", scale=1)

    # ç»‘å®šäº‹ä»¶
    refresh_btn.click(
        refresh_models,
        outputs=model_dropdown
    )

    load_btn.click(
        load_model,
        inputs=[model_dropdown],
        outputs=[status_display, msg]
    )

    unload_btn.click(
        unload_model,
        outputs=[status_display, msg]
    )

    submit_btn.click(
        chat_with_model,
        inputs=[msg, chatbot],
        outputs=[chatbot, msg]
    )

    msg.submit(
        chat_with_model,
        inputs=[msg, chatbot],
        outputs=[chatbot, msg]
    )

    clear_btn.click(
        clear_history,
        outputs=[chatbot, status_display]
    )

if __name__ == "__main__":
    # æ£€æŸ¥æ ¹è·¯å¾„
    if not os.path.exists(MODELS_ROOT_PATH):
        print(f"è­¦å‘Šï¼šæŒ‡å®šçš„æ¨¡å‹æ ¹è·¯å¾„ä¸å­˜åœ¨: {MODELS_ROOT_PATH}")
        print("è¯·åœ¨ä»£ç å¼€å¤´ä¿®æ”¹ MODELS_ROOT_PATH å˜é‡ä¸ºæ‚¨çš„å®é™…è·¯å¾„")

    print("å¯åŠ¨Gradioç•Œé¢...")
    print(f"æ¨¡å‹æ ¹è·¯å¾„: {MODELS_ROOT_PATH}")

    # æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹
    available_models = get_available_models()
    print(f"æ‰¾åˆ° {len(available_models)} ä¸ªæ¨¡å‹: {available_models}")

    demo.launch(
        server_name="localhost",
        server_port=7860,
        share=False
    )